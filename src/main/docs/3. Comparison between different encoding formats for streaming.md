# Comparison between different encoding formats for streaming

We are going to study the following data encoding formats in the context of streaming applications:
- Apache Avro
- Protocol Buffers
- JSON/JSONB
Considering next aspects:
- Strict high availability requirements: What elements need to be taken into account?
- Interoperability requirements: What elements need to be taken into account?
- Scalability and throughput requirements (both low latency and capacity)
- Support for schema evolution
- Compression and decompression rate: CPU required, compression rate
- Read and write performance
- Particular characteristics of each format that differentiate it from the rest.
- Other relevant aspects.

Let's start with high availability. For this aspect it would be important to talk about the size of the messages, here
Avro and Protobuf are efficient in terms of space, both using compact binary encoding, and it is JSON that is usually
more efficient as JSONB improves this aspect by storing data in binary. We could also take into account serialisation
and deserialisation speed, where Protobuf stands out, as it is in fact known for its high speed which decreases latency.
Then we could place Avro, which is also fast, and JSON again the slowest, with JSONB improving it a bit. At this point
we could also consider the interoperability and flexibility of the schema for its evolution but we will discuss this in
its corresponding points.

In terms of interoperability, Avro supports many programming languages, however, its design to work well with Hadoop may
limit its use in applications where simplicity is more important. In these, JSON is the most universal format, used on
almost all platforms and programming languages. It is also widely used in web services and REST APIs. Its flexibility and
readability make it the most preferred for interoperability despite its restrictions in other aspects. As for Protobuf,
it supports a wide range of languages and is widely used in distributed systems and microservices, especially within the
Google ecosystem.

In terms of performance and scalability, in addition to the previous two points, we could say that Avro is excellent
for applications that require scalability and performance in a Big Data environment. Protocol Buffers is ideal for
high-performance applications due to its efficient message size and processing speed. And JSON/JSONB is more suitable
where interoperability and readability are more important than performance.

In Avro, having the data schema stored with the data as we saw weeks ago ensures that the data and schemas are always
aligned. However, in Protobuf the schema is not included in the data, there is a centralised management, useful for
schema evolution in distributed environments. In contrast, JSON/JSONB does not have a fixed schema, which may be an
advantage in terms of flexibility in some cases but certainly complicates its management.

In terms of compression/decompression ratio and CPU required, Avro uses a compact binary encoding which generally
offers a good compression ratio, but Avro does not compress the data itself, it relies on compression algorithms to
do so, and makes use of a moderate amount of CPU. On the other hand, Protobuf also uses an encoding of the same type,
as we saw before, so the compression ratio is also good, and it is very CPU efficient. Finally, JSON is the least
efficient, but again JSONB improves this aspect. All in all, Protobuf has arguably the best compression ratio, followed
by Avro, well ahead of JSON/JSONB.
And finally, in write and read performance, Protocol Buffers takes the first place for its high performance, followed
by Avro, with very good results as well, and JSON/JSONB as we said talking about other aspects, this is not its strong
point.

And finally, in write and read performance, Protocol Buffers takes first place for its high performance,
followed by Avro, with very good results as well, and JSON/JSONB as we said talking about other aspects, this is not its
forte.

In general, we can highlight the high performance of Protobuf in writing and reading, the fact that in Avro the schema is
stored together with the data, and the flexibility and interoperability offered by JSON/JSONB. These features and the
others mentioned above allow us to choose the solution that best suits our needs, taking into account the strengths of
each of them and the importance of these aspects in our project.

### Bibliography:

Official documentation:
- Apache Avro: https://avro.apache.org/docs/
- Protocol Buffers: https://protobuf.dev/
- JSON: https://www.json.org/json-en.html

Other:
- https://medium.com/c-sharp-progarmming/schema-registry-in-kafka-avro-json-and-protobuf-9865b06c5b19#:~:text=Avro%20is%20suitable%20for%20use,complex%20data%20structures%20are%20needed.https://www.educative.io/answers/what-is-the-difference-between-json-and-jsonb-in-postgresql
- https://softwaremill.com/data-serialization-tools-comparison-avro-vs-protobuf/#overview-of-protobuf-and-avro
- https://blog.conan.io/2019/03/06/Serializing-your-data-with-Protobuf.htmlhttps://lab.wallarm.com/what/avro-vs-protobuf/

