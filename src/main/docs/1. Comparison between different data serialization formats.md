# Comparison between different data serialization formats

In this comparison, **Avro, Parquet, ORC, Delta Lake and Protobuf** data serialisation formats will be considered.

We will consider the following aspects:
 - Compression and decompression ratio: CPU needed, compression ratio, compression rate, etc.
 - Performance in writing and reading
 - Particular characteristics of each format that differentiate it from the rest
 - Other relevant aspects

It is obvious that the choice of the most suitable format for each project depends on several specific factors, such as
the volume of data, the type of data, the criticality of a good performance in both reading and writing, whether it is
preferable to give more resources to one function or another due to project requirements, the languages in which we are working...

We will first give an introduction to Protobuf and then move on to the comparison:

## Protocol Buffers

Protobuf (Protocol Buffer) was created by Google in 2001, and was released in 2008. It is a format used for data
serialisation and is open-source. The aim of its creation was to obtain a faster and smaller format than xml.
Its schemas are called messages and are reflected in a proto file (.proto), and are compiled with protoc. After
this compilation, code is generated that can be used. The compiler supports several programming languages, such as
C++, Java, Python and Ruby among others. It is intended for object-oriented programming languages. Although Scala is
not one of the officially supported languages, there is a third-party protoc for Scala, called ScalaPB, and using
SBT we can generate Scala case classes from a proto file. There is also an API that allows us to read and write messages.


In terms of general features, Protobuf is best suited for datasets that are no larger than a few megabytes and can be
loaded into memory from one. Also, there is no compression out of the box, but it is key if it is used in a context
where the data has to navigate through different data streams and languages.

## Comparison
Moving on to the comparison, depending on the nature of the data, if the data are in the form of tables, Parquet,
ORC, and Delta Lake (since the data are stored in Parquet format), which are more columnar formats, would be more
useful. However, if we have more record-type data, an Avro format would be more efficient.

On the other hand, if we had to evolve the schema of the data, for example, adding a column, or modifying another one,
Avro allows us to do it without touching the pipeline of the data, simply by generating a new version of this schema
and both versions could coexist. However, this is not possible in Protobuf, the format can only change when compiling,
and in the other formats, this conversion requires work.

Similarly, looking at the volume of data you have to work with, Delta Lake is not very recommendable for large writes that
have to be fast, the same goes for Parquet (for the reason mentioned above), and the same goes for ORC. However, ORC
turns out to have very good performance for reads. On the other hand, Avro is generally slower for both reads and
writes, and finally, Protobuf is fast as long as the data volume is small.

In terms of compression level, ORC has a high compression level which is why it is used in the cloud, the compression
in Parquet is good, and it is poorer in Avro. In protobuf there is no compression.

Finally, in terms of CPUs, Avro would need the least CPUs, followed by Parquet and Delta Lake, and lastly, ORC would
need the most.

To contextualise a little which formats would be the most recommendable depending on the part of the architecture:

- Ingest: although I have not mentioned JSON, this format is currently key in the ingest of data as it is widely
used in all environments. We could also consider AVRO at this stage, although only for more experienced architectures
to take advantage of the schema evolution feature.
- Storage: we could highlight parquet, for its efficient storage, and depending on the architecture in which we work,
Delta Lake if there is a lake and transactions are important, and ORC if we work in a Hadoop environment.
- Processing: parquet or ORC, we could also consider Delta Lake if we have a lakehouse with medallions. It is always
important to ensure compatibility with Apache Spark.
- Consumption: here again we include JSON for its popularity and easy interpretation, Parquet and Delta Lake for its
efficiency in reading to generate reports and dahsboards.

Protobuf is suitable for scenarios where low latency communication is needed and the data structure does not change
very frequently. It could be used for IoT, to send data between devices, such as sensors.

### Bibliography:

Official documentation:
- Avro: https://avro.apache.org/docs/
- Parquet: https://parquet.apache.org/docs/
- Delta Lake: https://docs.delta.io/latest/index.html
- ORC: https://orc.apache.org/docs/index.html
- Protobuf: https://protobuf.dev/overview/

Other:
- https://softwaremill.com/data-serialization-tools-comparison-avro-vs-protobuf/#overview-of-protobuf-and-avro
- https://medium.com/@prasku/computer-science-serialization-40ed973077a3
- https://blog.det.life/choosing-the-right-big-data-file-format-avro-vs-parquet-vs-orc-c868ffbe5a4e
- https://www.upsolver.com/blog/the-file-format-fundamentals-of-big-data
- https://scalapb.github.io/docs/getting-started
- https://blog.conan.io/2019/03/06/Serializing-your-data-with-Protobuf.htmlhttps://lab.wallarm.com/what/avro-vs-protobuf/
